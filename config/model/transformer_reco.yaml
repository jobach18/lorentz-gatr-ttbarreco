_target_: experiments.top_reco_transformer.wrappers.TopRecoTransformerWrapper

net:
  _target_: experiments.baselines.transformer.Transformer
  
  in_channels: 4        # Input 4-momenta (px, py, pz, E)
  out_channels: 128     # Output dimension per item (will be processed by wrapper)
  hidden_channels: 128  # Hidden dimension
  
  num_blocks: 4         # Number of transformer blocks
  num_heads: 8          # Number of attention heads
  dropout_prob: 0.1     # Dropout probability
  
  pos_encoding: false    # Use positional encoding
