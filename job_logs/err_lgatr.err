CUDA 11.8 loaded
/data/dust/user/bachjoer/beegfs.migration/mamba/lgatr/lib/python3.9/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.
  warnings.warn(msg, NumbaDeprecationWarning)
/data/dust/user/bachjoer/beegfs.migration/mamba/lgatr/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'top_reco': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
INFO  [lorentz-gatr] Set experiment topreco_local_debug with id 1
[2025-07-14 15:21:19 I] Creating new experiment topreco_local_debug/GATr_1186
[2025-07-14 15:21:19 D] Saving source to ./runs/topreco_local_debug/GATr_1186/source.zip
[2025-07-14 15:21:20 I] Set experiment topreco_local_debug with id 1
[2025-07-14 15:21:20 I] Using device cuda
[2025-07-14 15:21:20 I] ### Starting experiment topreco_local_debug/GATr_1186 (id=1) ###
[2025-07-14 15:21:21 W] Using training.force_xformers=False, this will slow down the network by a factor of 5-10.
/data/dust/user/bachjoer/beegfs.migration/mamba/lgatr/lib/python3.9/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
[2025-07-14 15:21:21 I] Instantiated model GATr with 402197 learnable parameters
[2025-07-14 15:21:21 I] Not using EMA
[2025-07-14 15:21:22 I] Creating TopRecoDataset from data/train_TTTo2L2Nu_train_scaled_genbot.npz
[2025-07-14 15:22:32 I] Storing scaling_factors_lambda4.json
[2025-07-14 15:23:09 I] Finished creating datasets after 107.30 s = 1.79 min
[2025-07-14 15:23:09 I] Constructed dataloaders with train_batches=2735, val_batches=2344, batch_size=256 (training), 128 (evaluation)
[2025-07-14 15:23:09 I] Starting to train for 15000 iterations = 5.5 epochs on a dataset with 2735 batches using early stopping with patience 100 while validating every 5000 iterations
/data/dust/user/bachjoer/beegfs.migration/mamba/lgatr/lib/python3.9/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.
  warnings.warn(msg, NumbaDeprecationWarning)
/data/dust/user/bachjoer/beegfs.migration/mamba/lgatr/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 2, 4])) that is different to the input size (torch.Size([1, 256, 2, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
[2025-07-14 15:23:14 I] Finished iteration 1 after 5.34s, training time estimate: 1335.08min = 22.25h
[2025-07-14 15:35:30 I] Finished iteration 1000 after 740.74s, training time estimate: 185.19min = 3.09h
/data/dust/user/bachjoer/beegfs.migration/mamba/lgatr/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([96, 2, 4])) that is different to the input size (torch.Size([1, 96, 2, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
[2025-07-14 16:24:36 I] ### Starting to evaluate model on val dataset ###
[2025-07-14 16:31:23 I] Evaluation time: 1357.79s for 1M events using batchsize 128
[2025-07-14 16:31:23 I] the true pt shape is (300000,)
[2025-07-14 16:31:23 I] the resolution shape is is ()
[2025-07-14 17:33:18 I] ### Starting to evaluate model on val dataset ###
[2025-07-14 17:39:05 I] Evaluation time: 1155.15s for 1M events using batchsize 128
[2025-07-14 17:39:05 I] the true pt shape is (300000,)
[2025-07-14 17:39:05 I] the resolution shape is is ()
[2025-07-14 18:40:29 I] ### Starting to evaluate model on val dataset ###
[2025-07-14 18:46:15 I] Evaluation time: 1152.96s for 1M events using batchsize 128
[2025-07-14 18:46:15 I] the true pt shape is (300000,)
[2025-07-14 18:46:15 I] the resolution shape is is ()
[2025-07-14 18:46:15 I] Finished training for 14999 iterations = 5.5 epochs after 203.10min = 3.39h
[2025-07-14 18:46:15 I] Loading model from ./runs/topreco_local_debug/GATr_1186/models/model_run0_it14999.pt
[2025-07-14 18:46:16 I] ### Starting to evaluate model on val dataset ###
[2025-07-14 18:52:02 I] Evaluation time: 1154.34s for 1M events using batchsize 128
[2025-07-14 18:52:02 I] the true pt shape is (300000,)
[2025-07-14 18:52:02 I] the resolution shape is is ()
[2025-07-14 18:52:04 I] Creating result_val_lambda4.json.
[2025-07-14 18:52:14 I] Creating plots in ./runs/topreco_local_debug/GATr_1186/plots_0
[2025-07-14 18:52:17 I] GPU RAM information: max_used = 9.05 GB, max_total = 17.1 GB
[2025-07-14 18:52:17 I] Finished experiment topreco_local_debug/GATr_1186 after 210.94min = 3.52h
[2025-07-14 18:52:17 I] the returned result of the mlflow run is 13.414169311523438
