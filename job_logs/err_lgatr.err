CUDA 11.8 loaded
/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.
  warnings.warn(msg, NumbaDeprecationWarning)
/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'top_reco': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
INFO  [lorentz-gatr] Set experiment topreco_local_debug with id 1
[2024-11-14 13:28:40 I] Creating new experiment topreco_local_debug/GATr_0289
[2024-11-14 13:28:40 D] Saving source to ./runs/topreco_local_debug/GATr_0289/source.zip
[2024-11-14 13:28:49 I] Set experiment topreco_local_debug with id 1
[2024-11-14 13:28:49 I] Using device cuda
[2024-11-14 13:28:49 I] ### Starting experiment topreco_local_debug/GATr_0289 (id=1) ###
[2024-11-14 13:29:12 W] Using training.force_xformers=False, this will slow down the network by a factor of 5-10.
/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
[2024-11-14 13:29:13 I] Instantiated model GATr with 65955 learnable parameters
[2024-11-14 13:29:13 I] Not using EMA
[2024-11-14 13:29:13 I] Creating TopRecoDataset from data/train_TTTo2L2Nu_train.npz
[2024-11-14 13:30:00 I] Finished creating datasets after 47.28 s = 0.79 min
[2024-11-14 13:30:00 I] Constructed dataloaders with train_batches=5469, val_batches=2344, batch_size=128 (training), 128 (evaluation)
[2024-11-14 13:30:00 I] Starting to train for 10 iterations = 0.0 epochs on a dataset with 5469 batches using early stopping with patience 100 while validating every 5000 iterations
[2024-11-14 13:30:01 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.
  warnings.warn(msg, NumbaDeprecationWarning)
[2024-11-14 13:30:05 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] Finished iteration 1 after 5.61s, training time estimate: 0.93min = 0.02h
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:06 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:06 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:07 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:07 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:07 I] network inputs 
multivectors=torch.Size([1, 1792, 1, 16]) and scalar=torch.Size([1, 1792, 0])
[2024-11-14 13:30:07 I] network output with 
multivector=torch.Size([1, 1792, 1, 16]), outputs=torch.Size([256, 1, 4]) 
[2024-11-14 13:30:07 I] Finished training for 9 iterations = 0.0 epochs after 0.11min = 0.00h
[2024-11-14 13:30:07 W] Cannot load best model (epoch 0) from ./runs/topreco_local_debug/GATr_0289/models/model_run0_it0.pt
[2024-11-14 13:30:07 I] ### Starting to evaluate model on val dataset ###
[2024-11-14 13:30:07 E] Exiting with error
Traceback (most recent call last):
  File "/beegfs/desy/user/bachjoer/ml-workspace/lgatr/experiments/base_experiment.py", line 42, in __call__
    self.run_mlflow()
  File "/beegfs/desy/user/bachjoer/ml-workspace/lgatr/experiments/base_experiment.py", line 64, in run_mlflow
    self.full_run()
  File "/beegfs/desy/user/bachjoer/ml-workspace/lgatr/experiments/base_experiment.py", line 91, in full_run
    self.evaluate()
  File "/beegfs/desy/user/bachjoer/ml-workspace/lgatr/experiments/top_reco/experiment.py", line 114, in evaluate
    self.results[set_label] = self._evaluate_single(
  File "/beegfs/desy/user/bachjoer/ml-workspace/lgatr/experiments/top_reco/experiment.py", line 136, in _evaluate_single
    pred = self.model(
  File "/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/beegfs/desy/user/bachjoer/ml-workspace/lgatr/experiments/top_reco/wrappers.py", line 54, in forward
    multivector = embedding["mv"].unsqueeze(0)
  File "/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/torch_geometric/data/batch.py", line 175, in __getitem__
    return super().__getitem__(idx)
  File "/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/torch_geometric/data/data.py", line 498, in __getitem__
    return self._store[key]
  File "/beegfs/desy/user/bachjoer/mamba/lgatr/lib/python3.9/site-packages/torch_geometric/data/storage.py", line 111, in __getitem__
    return self._mapping[key]
KeyError: 'mv'
